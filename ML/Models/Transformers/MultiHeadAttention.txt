MultiHeadAttention - это механизм распараллеливания операции вычисления 
		     attention значений по головам(head), что ускоряет 
		     работу трансформера

Алгоритм:
	1. Вместо матриц W_q, W_k, W_v размерностью MxD используются матрицы
	   размера MxD/NxN

	2. После проведения всех операций и вычисления внимания по головам
	   происходит конкатенация результатов

	3. После конкатенации умножаем результат на доп матрицу W_0

Статьи:
 - https://paperswithcode.com/method/multi-head-attention
