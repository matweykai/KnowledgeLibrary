Трансформер - это encoder-decoder архитектура, которая была придумана для 
	      решения задач seq-to-seq.

Особенности трансформера:
 * Использует мощь механизма внимания на полную катушку
 * Устойчив к перестановке слов, благодаря positional encoding
 * Multi-Head-Attention позволяет параллелить модель и ускорять вычисления
 * Требователен к ресурсам (память)

Основной алгоритм:
	1. Получение Embedding'ов для входящих токенов

	2. Добавление(сложение) PositionalEncoding к входному вектору

	3. Получение матриц K, Q, V из входной последовательности(матрица)

	4. Multi-Head-Attention над матрицами (3)

	5. Skip-Connection (4) и (2) с нормализацией (LayerNorm)

	6. 2 полносвязных слоя с ReLU (FeedForward)

	7. Skip-Connection (6) и (5) с нормализацией

	### Попадаем в decoder ###

	8. На вход decoder получает  выходную последовательность, которую он 
	   сгенерировал на предыдущих шагах
	 
	9. Positional Encoding для выходной последовательности

	10. Masked-Multi-Head-Attention
	    * Правая часть выходных токенов(которых ещё нет) получает значения
	     -inf, а в последствии 0 внимания decoder'a

	11. Skip-Connection (9) и (10) с нормализацией

	12. Multi-Head-Attention, где Query и Key - это выход с encoder'a,
	    а Value - из (11)

	13. Skip-Connection (11) и (12) с нормализацией

	14. 2 полносвязных слоя с ReLU (Feed Forward)

	15. Skip-Connection (13) и (14)

	16. Полносвязный слой

	17. Softmax

	18. Train: конец. 
	    Inference: Получили один вектор. Теперь возвращаемся к 8 шагу
	    * Почему для train конец? Потому что при обучении у нас есть 
	      выходная последовательность и мы её подаём декодеру как вход и
	      маскируем. А при Inference нужно в цикле генерировать 
	      последовательности слово за словом.

Модификации трансформера:
 * BERT - encoder для текстов, предложений, слов
 * ViT - трансформер для задач компьютерного зрения

Статьи:
 - https://medium.com/swlh/what-exactly-is-happening-inside-the-transformer-b7f713d7aded
 - https://kikaben.com/transformers-encoder-decoder/
 - https://jalammar.github.io/illustrated-transformer/
