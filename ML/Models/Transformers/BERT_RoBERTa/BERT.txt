BERT(Bidirectional Encoder Representations from Transformers) - это модель для
		генерации embedding'ов для токенов, которая была разработана
		Google

* BERT показал отличные результаты в задачах NLP. С помощью него можно решать
  задачи классификации, NER, QuestionAnswering и тд.

* Сам по себе BERT - это просто Encoder из классической трансформерной 
  архитектуры, который был обучен с помощью MLM и NSP подхода

* При работе с BERT исследователи Google вывели некоторые рекомендации:
  1. С ростом длительности обучения, качество растёт
  2. Чем больше модель, тем более хорошие результаты она показывает
  3. При обучении лучше комбинировать MLM и NSP подходы

* В основном BERT используется как Backbone и просто дотренировывается 
  последний слой классификации

Статьи:
 - https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
