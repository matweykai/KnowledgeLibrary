RoBERTa(Robustly Optimized BERT) - это улучшение BERT от FaceBook

* Архитектурно данные модели идентичны, различия есть только в методе обучения

* Исследователи их FaceBook решили, что BERT не исчерпал свой потенциал и:
	1. Добавили данных 

	2. Увеличили размеры батчей при обучении

	3. Не использовали NSP

	4. Увеличили размеры последовательностей

	5. Сменили статическое маскирование на динамическое

Статьи:
 - https://medium.com/dataseries/roberta-robustly-optimized-bert-pretraining-approach-d033464bd946
