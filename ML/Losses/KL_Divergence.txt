KL дивергенция - это функционал потерь, который может использоваться для
		 обучения моделей. Он был назван в честь Кульбака Лейблера

* Данный функционал является ассиметричным и позволяет сравнивать два
  распределения между собой

* Этот функционал отличается от кросс энтропии всего лишь на константу, поэтому
  она чаще всего используется вместо KL дивергенции

Формула:

	KL (P || Q) = sum (P(X) * log(P(X) / Q(X)))

	P(X) - вероятность пронаблюдать событие X в распределении P
	Q(X) - вероятность пронаблюдать событие X в распределении Q

* Чтобы вывести кросс энтропию сделаем несколько шагов:

	1) P(X) * log(P(X) / Q(X)) = P(X) * log(P(X)) - P(X) * log(Q(X))
	2) sum (P(X) * log(P(X) / Q(X))) = (*)
	   (*) = - sum(P(X)*log(Q(X))) - sum(-P(X) * log(P(X)))

	3) (*) = cross_entropy(P, Q) + sum(P(X) * log(P(X)))

* Данная функция потерь часто используется в задачах отслеживания смещения
  данных, а также GAN и VAE моделях

Полезные ссылки:
 - https://encord.com/blog/kl-divergence-in-machine-learning/
