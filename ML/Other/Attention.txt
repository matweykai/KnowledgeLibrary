Attention(или механизм внимания) - это интересный механизм, который
		преимущественно используется в трансформерных 
		архитектурах, хотя его аналоги есть и в Faster RCNN
		(в виде RPN)

* Под данным определением понимают архитектурный механизм, который
  имитирует внимание человека. Т.е сеть сама выбирает нужные ей 
  участки последовательностей данных

* В общем случае механизм внимания это несколько операций матричного
  умножения и функции Softmax

* Краткое пояснение к данному механизму: "мы рассчитываем 
  релевантность какой-либо информации и агрегируем ее"

* Обычно вектора называются Q, K и V:
  Query: X * W_Q = Q - это то что будет умножаться на Key
  Key: X * W_K = K
  Value: X * W_V = V - это то что будет умножаться на Softmax от
	 произведения предыдущего слоя

Общая формула:
	Attention = f(g(x), x), 
		где g(x) - генератор attention-score'ов
		    f(g(x), x) - некоторая функция применения 
				 внимания к исходным данным

	Softmax(Q * K_T) * V = Result - self attention

* Из данной формулы видно, что Softmax распределяет внимание по
  входным данным

* Существуют разные типы механизмов внимания:
  - Поканальный (Channel Attention)
  - Пространсвенный (Spatial Attention)
  - Временной механизм внимания (Temporal Attention)
  - Блочный механизм внимания (Branch attention)

Статьи:
 - https://telegra.ph/CHto-takoe-attention-12-09
 - https://arxiv.org/pdf/1706.03762.pdf
 - https://t.me/deep_school/106